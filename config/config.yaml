## Hydra Settings ##
defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  run:
    dir: .
  output_subdir: null
  sweep:
    dir: .
    subdir: .

## Experiment Tracker Settings ##
tracker:
  name: mlflow
  config_name: config.yaml
  data_dir: ${root}
  model_name: ${net.model_name}
  wandb:
    project: typhoon
    run_name: convnextv2_tiny.fcmae_ft_in22k_in1k_384-fold0
    tags: []
    notes: ""
  mlflow:
    uri: null
    experiment: typhoon
    run_name: convnextv2_tiny.fcmae_ft_in22k_in1k_384-fold0

## User Settings ##
root: data/typhoon/train
df_path: inputs/train_val_kfold_split_downsampling.csv
fold: 0 # 0 - 4
input_channels: 1 # Gray Scale
num_classes: 2
image_size: 64
seed: 0
label_map: {
  nonTC: 0,
  TC: 1,
}

## Dataset ##
train_dataloader:
  batch_size: 16
  shuffle: True
  num_workers: 10
  pin_memory: True
  imbalancedDatasetSampler: True

val_dataloader:
  batch_size: 512
  shuffle: False # DO NOT CHANGE!!!
  num_workers: 10
  pin_memory: True

test_dataloader:
  batch_size: 1
  shuffle: False # DO NOT CHANGE!!!
  num_workers: 10
  pin_memory: True

train_transform:
  random_crop:
    enable: True
    image_size: ${image_size}
  randaugment:
    enable: False
    num_ops: 4
    magnitude: 9
  trivial_augment_wide:
    enable: False
  augmix:
    enable: True
    severity: 3
    mixture_width: 3
    chain_depth: -1
    alpha: 1.0
    all_ops: True
  normalize:
    enable: True
    mean: [0.5]
    std: [0.5]
  cutmix_mixup:
    enable: True
    mixup:
      alpha: 0.4
    cutmix:
      alpha: 1.0
    max_epochs: 20

test_transform:
  center_crop:
    enable: True
    image_size: ${image_size}
  normalize:
    enable: True
    mean: [0.5]
    std: [0.5]

## Model ##
net:
  model_name: convnextv2_tiny.fcmae_ft_in22k_in1k_384 # timm.model
  pretrained: True

metrics:
  task: multiclass
  top_k: 1
  average: macro
  f_beta_weight: 0.5

loss_fn:
  name: CrossEntropyLoss
  focal:
    gamma: 2
    reduction: mean

optimizer:
  name: AdamW
  adam:
    lr: 1e-4
    weight_decay: 1e-5
  adamW:
    lr: 1e-4
    weight_decay: 1e-5
  sgd:
    lr: 1e-4
    momentum: 0.9
    weight_decay: 0
  ranger21:
    lr: 1e-4
    weight_decay: 1e-5
    num_batches_per_epoch: 54 # dataset / batch size
  sam:
    base_optimizer: AdamW
    rho: 0.05
    adaptive: False

scheduler:
  name: CosineAnnealingWarmRestarts
  CosineAnnealingWarmRestarts:
    T_0: 10
    eta_min: 1e-6

callbacks:
  early_stopping:
    enable: True
    monitor: val_MulticlassFBetaScore
    patience: 20
    mode: max
  model_checkpoint:
    enable: True
    monitor: val_MulticlassFBetaScore
    mode: max
    save_top_k: 1
    save_last: False

## Trainer ##
trainer:
  max_epochs: 50
  accelerator: gpu
  devices: auto
  accumulate_grad_batches: 16
  deterministic: True
  ckpt_path:
